---
title: "Inflation Insight: Forecasting Australian Inflation Rate Using Bayesian VAR model"
author: "Gezhi Chen"
format:
  html:
    toc: true
    toc-location: left
    number-sections: true

execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.** The purpose of this research is to explore the trend in Australia's inflation rate over the next two years using the Bayesian Vector Auto-regression (BVAR) model.

> **Keywords.** bsvars, quarto, R, Australia inflation rate

## Introduction

### Question Objective and Motivation

**Question:**

Will Australia's inflation rate fall back to the 2-3% inflation target range in 2025?

**Objective and Motivation:**

Inflation has always been a topic of interest for economists, as trends in the inflation rate provide essential guidance for key decisions by economic participants, thus playing a crucial role in the economic and social development of a country [@bernoth2021]. The Reserve Bank of Australia (RBA) forecasts that by 2025, inflation will fall back to the target range of 2% to 3% and reach the midpoint of this range by 2026. Influenced by the COVID-19 pandemic, Blot et al. deduced that the inflation rate is affected by a variety of factors, such as GDP, exchange rates, interest rates, and unemployment rates [@EuropeanParliament.2022]. Therefore, this study will assess the reasonableness of the RBA's inflation rate forecast for the next two years. During the COVID-19 period, Australia's inflation rate sharply rose to 7.8%, in response to which the RBA implemented a monetary tightening policy and began raising interest rates at the end of 2022, slowing the pace of rate hikes in the latter half of 2023. Hence, this report will further discuss whether the RBA has correctly assumed that its monetary policy measures have effectively curbed inflation.

```{r Start}
rm(list=ls())
```

```{r Library}
#| echo: false
#| message: false
#| warning: false

library(readabs)
library(readrba)
library(xts)
library(dplyr)
library(lubridate)    # quarterly data
library(corrplot)     # cor plot
library(fUnitRoots)   # ADF test - adfTest
library(tidyverse)    # for table
library(kableExtra)   # for print table
library(zoo)
library(ggplot2)      # plot in same graph
library(GIGrvg)       # GIG distribution
library(mvtnorm)      # Bvars forecast
library(plot3D)
library(HDInterval)   # hdi plot
library(MASS)
library(mgcv)
```

```{r Colour}
#| echo: false
#| message: false
#| warning: false

blue1 = "blue"            # #0000FF
blue2 = "lightblue"       # #ADD8E6
blue3 = "royalblue"       # #4169E1
blue4 = "deepskyblue"     # #00BFFF
blue5 = "dodgerblue"      # #1E90FF
blue6 = "steelblue"       # #4682B4


green1  = "#05386B"
green2  = "#379683"
green3  = "#5CDB95"
green4  = "#8EE4AF"
green5  = "#EDF5E1"
green6  = "darkgreen"


blue4.rgb   = col2rgb(blue4)
blue4.shade1= rgb(blue4.rgb[1],blue4.rgb[2],blue4.rgb[3], alpha=120, maxColorValue=255)
green3.rgb   = col2rgb(green3)
green3.shade1= rgb(green3.rgb[1],green3.rgb[2],green3.rgb[3], alpha=120, maxColorValue=255)
```

## Data and Data Properties

### Data Selection and Rationale

-   **Direct Inflationary Indicators:**

    1.$cpi_t$: Consumer Price Index (CPI) from ABS.

    CPI directly measure inflation by follow formula:

    $$Inflation  = (\frac{CPI_{Quarter \ of \ These \ Year}}{CPI_{Quarter \ of \ Previous \ Year} } - 1 )\times 100$$ {#eq-1}

    CPI is the basic data for measuring inflation. It is more stable and less affected by seasonal factors and short-term fluctuations than the annual inflation rate from which it is derived. In VAR, raw time series data rather than rates of change are used to capture and model the dynamic nature of the data.

    2.$infexp_t$: Business inflation expectations -- 3-months ahead from RBA

    Inflation expectations often guide consumer and business behavior; if inflation is expected to rise, they might make purchases and raise prices in advance, thus driving actual inflation up in the short term. Meanwhile, central banks closely monitor inflation expectations, adjusting monetary policies to influence these expectations and control actual inflation to maintain price stability.

-   **Economic Activity and Policy Indicators:**

    3.  $gdp_t$: Gross Domestic Product (GDP) from ABS.

    GDP reflects the size and growth rate of a country's economy. When GDP increases, it indicates increased economic activity, which can lead to demand-pull inflation because increased demand may exceed current production capacity, pushing up prices.

    4.  $crt_t$: Cash rate target published by RBA.

    This is the main tool used by the RBA to influence economic activity. Raising the cash rate is usually aimed at reducing borrowing and spending, thereby reducing inflationary pressures.

    5.  $unemp_t$: Unemployment rate from ABS.

    It indicate the level of slack in the labor market, influencing wage-push inflation.

    6.  $m_t$: Money aggregate (Broad money) from RBA.

    'Broad money' is defined as 'M3' plus 'Other borrowings from private sector by AFIs'. When the growth rate of broad money supply in an economy exceeds the growth rate of its real output, it leads to more money chasing the same amount of goods and services, causing price levels to rise, i.e., inflation. Therefore, central banks like the Reserve Bank of Australia (RBA) adjust interest rates and other tools to control the growth of broad money to achieve their inflation targets.

-   **Market and External Trade Indicators:**

    7.  $export_t$: International exports from ABS.

    It affects trade balance and currency strength, influencing imported inflation.

    8.  $import_t$: International imports from ABS.

    Directly affect inflation through the cost of imported goods.

    9.  $aord_t$: All Ordinaries Index (AORD) from yahoo finance.

    It reflects investor confidence and economic activity which can be pre-emptive indicators of inflation.

    10. $exr_t$: AUD/USD exchange rate from RBA.

    It affects the price of imports and exports, contributing to inflation.

```{r Data Downloading, fig.pos="H"}
#| echo: false
#| message: false
#| warning: false


### Data downloading

# 1.Inflation / CPI 
# 6401.0 Consumer Price Index, Australia
# series_id = "A2325846C": Index Numbers ;  All groups CPI ;  Australia ;
cpi_download      = read_abs(series_id = "A2325846C")     
cpi_data          = xts(cpi_download$value, cpi_download$date)
correct_dates     = seq(as.Date("1948-10-01"), by="quarter", length.out=length(cpi_data))
correct_dates     = floor_date(correct_dates, "month") - days(1)
index(cpi_data)   = correct_dates


# 2.GDP
# 5206.0 Australian National Accounts: National Income, Expenditure and Product
# series_id = "A2304404C": GDP per capita: Chain volume measures ;
gdp_download      = read_abs(series_id = "A2304404C")     
gdp_data          = xts(gdp_download$value, gdp_download$date)
correct_dates     = seq(as.Date("1959-10-01"), by="quarter", length.out=length(gdp_data))
correct_dates     = floor_date(correct_dates, "month") - days(1)
index(gdp_data)   = correct_dates


# 3.Cash rate target

crt_download      = read_rba(series_id = "FIRMMCRTD")   
crt_data          = xts(crt_download$value, crt_download$date)
quarter_ends      = endpoints(crt_data , on = "quarters")
crt_data          = crt_data[quarter_ends]
correct_dates     = seq(as.Date("1990-04-01"), by="quarter", length.out=length(crt_data))
correct_dates     = floor_date(correct_dates, "month") - days(1)
index(crt_data)   = correct_dates


# 4.Unemployment rate
# 6202.0 Labour Force, Australia
# series_id = "A84423050A": Unemployment rate ;  Persons ; seasonal adjust
unemp_download      = read_abs(series_id = "A84423050A")     
unemp_data          = xts(unemp_download$value, unemp_download$date)
quarter_ends        = endpoints(unemp_data , on = "quarters")
unemp_data          = unemp_data[quarter_ends]
correct_dates       = seq(as.Date("1978-04-01"), by="quarter", length.out=length(unemp_data))
correct_dates       = floor_date(correct_dates, "month") - days(1)
index(unemp_data)   = correct_dates


# 5.Export
# 5368.0 International Trade in Goods
# series_id = "A2718603V": Debits, Total goods ;
export_download     = read_abs(series_id = "A2718603V")     
export_data         = xts(export_download$value, export_download$date)
export_data         = abs(export_data)
quarter_ends        = endpoints(export_data , on = "quarters")
export_data         = export_data[quarter_ends]
correct_dates       = seq(as.Date("1971-10-01"), by="quarter", length.out=length(export_data))
correct_dates       = floor_date(correct_dates, "month") - days(1)
index(export_data)  = correct_dates


# 6.Import
# 5368.0 International Trade in Goods
# series_id = "A2718577A": Credits, Total goods ;
import_download     = read_abs(series_id = "A2718577A")     
import_data         = xts(import_download$value, import_download$date)
quarter_ends        = endpoints(import_data , on = "quarters")
import_data         = import_data[quarter_ends]
correct_dates       = seq(as.Date("1971-10-01"), by="quarter", length.out=length(import_data))
correct_dates       = floor_date(correct_dates, "month") - days(1)
index(import_data)  = correct_dates

# 7. Broad money
# D3 MONETARY AGGREGATES
# Series ID: DMABMN: Broad money – For series breaks see Series Breaks 'Broad money' is defined as 'M3' plus ‘Other borrowings from private sector by AFIs.

m_download        = read_rba(series_id = "DMABMN")   
m_data            = xts(m_download$value, m_download$date)
quarter_ends      = endpoints(m_data , on = "quarters")
m_data            = m_data[quarter_ends]
correct_dates     = seq(as.Date("1976-10-01"), by="quarter", length.out=length(m_data))
correct_dates     = floor_date(correct_dates, "month") - days(1)
index(m_data)     = correct_dates


# 8. Business inflation expectations – 3-months ahead
# G3 INFLATION EXPECTATIONS 
# Series ID: GBUSEXP: Survey measure of business expectations; Increase in final prices for 3-months ahead; Annualised

infexp_download      = read_rba(series_id = "GBUSEXP")   
infexp_data          = xts(infexp_download$value, infexp_download$date)
quarter_ends         = endpoints(infexp_data , on = "quarters")
infexp_data          = infexp_data[quarter_ends]
correct_dates        = seq(as.Date("1989-10-01"), by="quarter", length.out=length(infexp_data))
correct_dates        = floor_date(correct_dates, "month") - days(1)
index(infexp_data)   = correct_dates

# 9.AORD
aord_link           = "https://query1.finance.yahoo.com/v7/finance/download/%5EAORD?period1=460339200&period2=1712793600&interval=1mo&filter=history&frequency=1mo&includeAdjustedClose=true"
aord_download       = read.csv(aord_link)
aord_data           = data.frame(aord_download[,1], aord_download[,6])
colnames(aord_data) = c('date', 'aord')
aord_data$date      = as.Date(as.character(aord_data$date),format="%Y-%m-%d") 
# aord_data$aord      = as.numeric(na_if(aord_data$aord, "null"))
aord_data           = na.omit(aord_data)
aord_data           = xts(aord_data$aord, aord_data$date)
aord_data           = to.quarterly(aord_data, OHLC = FALSE)
correct_dates       = seq(as.Date("1985-04-01"), by="quarter", length.out=length(aord_data))
correct_dates       = floor_date(correct_dates, "month") - days(1)
index(aord_data)    = correct_dates

# 10. AUD/USD

exr_download      = read_rba(series_id = "FXRUSD", frequency("Monthly"))   
exr_data          = xts(exr_download$value, exr_download$date)
quarter_ends      = endpoints(exr_data, on = "quarters")
exr_data          = exr_data[quarter_ends]
correct_dates     = seq(as.Date("1969-10-01"), by="quarter", length.out=length(exr_data))
correct_dates     = floor_date(correct_dates, "month") - days(1)
index(exr_data)   = correct_dates

```

```{r Data Combine, fig.pos="H"}
#| echo: false
#| message: false
#| warning: false

# All Variables
all_data             = na.omit(merge(cpi_data,    infexp_data, 
                                     gdp_data,    crt_data, 
                                     unemp_data,  m_data,  
                                     export_data, import_data, 
                                     aord_data,   exr_data ))

colnames(all_data)   = c("cpi_data",    "infexp_data", 
                         "gdp_data",    "crt_data", 
                         "unemp_data",  "m_data", 
                         "export_data", "import_data",
                         "aord_data",   "exr_data")
```

Following Figure 1 shows the correlation between CPI and other variables.

Despite the low direct correlation between inflation expectations and CPI, inflation expectations may more closely reflect concerns about core inflation, which excludes volatile items like food and energy, whereas the CPI includes all items. If the trends between core inflation and overall inflation differ, this discrepancy can reduce the correlation between inflation expectations and the CPI. However, inflation expectations provide additional insight into how future economic conditions are perceived by consumers, businesses, and investors. This forward-looking perspective is important as it might influence economic decisions that are not immediately apparent through current CPI data.

Exchange rate fluctuations are typically sensitive to immediate market sentiment and short-term capital flows, whereas the CPI, as a measure of past price level changes, often exhibits a lag in responding to market changes. Even if the correlation between the CPI and the AUD/USD exchange rate is low, including this variable can help researchers understand economic changes from a broader perspective. For example, changes in the exchange rate can indirectly affect domestic price levels through the cost of imports, especially in open economies.

It should be noted that the unemployment rate exhibits a weak correlation with the CPI. This could be attributed to the Phillips Curve, which illustrates a short-term inverse relationship between unemployment and inflation. Over the long term, however, this correlation may diminish due to various influencing factors. For instance, following COVID, the inflation rate in the United States has been continuously rising due to the bankruptcy crisis among small and medium-sized enterprises (SMEs), part of which is attributed to the inadequate supply of export goods produced by SMEs leading to a decline in export volumes, thereby driving up the domestic inflation rate in the US [@kalemli-ozcan2020]. This impact can be seen as an indirect effect of rising unemployment rates on inflation. Therefore, despite the weak correlation between unemployment rate and CPI, it is still important to include it in the model for a comprehensive analysis.

```{r Corr Plot, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 1 Correlation matrix"

## plot corr table
cor_matrix          = round(cor(all_data), 4)

corrplot(cor_matrix, method = "color", 
         type = "upper", 
         tl.srt = 45,    
         tl.col = "black",
         tl.cex = 0.6,  
         col = colorRampPalette(c("darkblue", "#FFFFFF", "darkred"))(200), 
         addCoef.col = "darkgrey", 
         number.cex = 0.5,     
         diag = FALSE, 
         title = "Correlation Matrix",
         mar = c(0, 0, 2, 0),
         cl.lim = c(-1, 1), 
         cl.cex = 0.3, 
         cl.ratio = 0.1
)
```


In summary, the dataset included data from 1990Q2 to 2024Q1, total 137 observation points with 10 variables.

### Data Transformation

-   **Quarterly Transformation**

All data have been quarterly converted by selecting the data on the last day of the quarter as the observation. One of the reasons for not using monthly data is that high-quality quarterly economic data are more readily available compared to monthly data. In this report, the key data, CPI, which published on ABS, and the monthly data starting point is Q3 of 2017. Since the research wanted to capture longer-term data, quarterly data was considered. Also, policymakers and economists often rely more on quarterly data for decision-making because it provides a more stable and comprehensive view of economic conditions to some extent. This stability is key to understanding and predicting economic trends, especially when considering the long-term impacts of policies.

-   **Log Transformation**

Based on the line graph of the original data below (Figure 2), we can observe that exponentially growing variables need to be linearized, such as $cpi_t$, $gdp_t$, $m_t$, $export_t$, $import_t$, $aord_t$. Therefore, its logarithmically transformed form will be used in the following analysis.

```{r Data plot, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false

### Data plot

## Line plot for original data
par(mfcol = c(5, 2), mar = c(2, 2, 2, 2))

for (i in 1:10) {
  ts.plot(all_data[, i], main = colnames(all_data)[i], 
          ylab = "", xlab = "", col = blue1)
}
```

Figure 2 Line plot of original data

```{r Log transfomation}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 2 Line plot of original data"

## log transformation for exp data

lcpi_data     = log(cpi_data)
lgdp_data     = log(gdp_data)
lm_data       = log(m_data)
lexport_data  = log(export_data)
limport_data  = log(import_data)
laord_data    = log(aord_data)

# All Variables after log
all_data             = na.omit(merge(lcpi_data,     infexp_data, 
                                     lgdp_data,     crt_data, 
                                     unemp_data,    lm_data,  
                                     lexport_data,  limport_data, 
                                     laord_data,    exr_data ))

colnames(all_data)   = c("lcpi_data",      "infexp_data", 
                         "lgdp_data",      "crt_data", 
                         "unemp_data",     "lm_data", 
                         "lexport_data",   "limport_data",
                         "laord_data",     "exr_data")
```

-   **Integration transformation**

According to ACF plot for all data (Figure 3), we can see all data with autocorrelation. It suggests that the series is not white noise and might not be stationary.

```{r ACF plot, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 3 ACF plot for all data"

## ACF plot
par(mfcol = c(5, 2), mar=c(2,2,2,2))
for (i in 1:10){
  acf = acf(all_data[,i], plot = FALSE)[1:20]
  plot(acf, main = "")
  title(main = paste(colnames(all_data)[i]), line = 0.5)
}
```

Considering the selection of the order of single-order integration, the unit root test (ADF test) is used and the p value is used to determine its significance. A small p-value means the null hypothesis is rejected (non-stationary).

According to Table 1, we can see that $infexp_t$, $lm_t$, require more than first order integration, while other require first-order integration(under 5% level of confidence).

```{r ACF test, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| tbl-cap: "Table 1 Unit root test for all data"

## AR

# find the optimal lag 
ar_results <- list()

for (i in 1:ncol(all_data)) {
  ol.aic.ar <- ar(all_data[,i], order.max=20, aic=TRUE, method="ols")
  
  ar_results[[colnames(all_data)[i]]] <- ol.aic.ar$order
}

## ADF test

# ol.cpi.aic.ar$order
adf.cpi   = adfTest(all_data[,1], lags=5, type="c")              # don't reject -> non-stationary
dadf.cpi  = adfTest(diff(all_data[,1]), lags=4, type="nc")        # don't reject -> non-stationary

# ol.infexp.aic.ar$order
adf.infexp   = adfTest(all_data[,2], lags=5, type="c")               # don't reject -> non-stationary
dadf.infexp  = adfTest(diff(all_data[,2]), lags=4, type="nc")        # reject -> (I1 is stationary)

# ol.gdp.aic.ar$order
adf.gdp   = adfTest(all_data[,3], lags=7, type="c")              # don't reject -> non-stationary
dadf.gdp  = adfTest(diff(all_data[,3]), lags=6, type="nc")       # reject -> (I1 is stationary)

# ol.crt.aic.ar$order
adf.crt  = adfTest(all_data[,4], lags=20, type="c")               # don't reject -> non-stationary
dadf.crt = adfTest(diff(all_data[,4]), lags=19, type="nc")        # reject -> (I1 is stationary)

# ol.unemp.aic.ar$order
adf.unemp  = adfTest(all_data[,5], lags=5, type="c")           # don't reject -> non-stationary
dadf.unemp = adfTest(diff(all_data[,5]), lags=4, type="nc")    # reject -> (I1 is stationary)


# ol.m.aic.ar$order
adf.m      = adfTest(all_data[,6], lags=5, type="c")           # don't reject -> non-stationary
dadf.m     = adfTest(diff(all_data[,6]), lags=4, type="nc")    # don't reject -> non-stationary

# ol.export.aic.ar$order
adf.export  = adfTest(all_data[,7], lags=6, type="c")           # don't reject -> non-stationary
dadf.export = adfTest(diff(all_data[,7]), lags=5, type="nc")    # reject -> (I1 is stationary)

# ol.import.aic.ar$order
adf.import  = adfTest(all_data[,8], lags=1, type="c")           # don't reject -> non-stationary
dadf.import = adfTest(diff(all_data[,8]), lags=0, type="nc")    # reject -> (I1 is stationary)

# ol.aord.aic.ar$order
adf.aord   = adfTest(all_data[,9], lags=1, type="c")               # don't reject -> non-stationary
dadf.aord  = adfTest(diff(all_data[,9]), lags=0, type="nc")        # reject -> (I1 is stationary)


# ol.exr.aic.ar$order
adf.exr   = adfTest(all_data[,10], lags=1, type="c")               # don't reject -> non-stationary
dadf.exr  = adfTest(diff(all_data[,10]), lags=0, type="nc")        # reject -> (I1 is stationary)

Unit_Root_Test_table <- 
  tibble( " " = c("lcpi", "infexp", "lgdp", "crt", "unemp", "lm", "lexport", "limport", "laord", "exr"),
          "p value of ADF test of AR" 
          = round(c(adf.cpi@test$p.value,     adf.infexp@test$p.value,
                    adf.gdp@test$p.value,     adf.crt@test$p.value,    
                    adf.unemp@test$p.value,   adf.m@test$p.value,
                    adf.export@test$p.value,  adf.import@test$p.value,
                    adf.aord@test$p.value,    adf.exr@test$p.value),4),
          "p value of ADF test of diff-AR" 
          = round(c(dadf.cpi@test$p.value,    dadf.infexp@test$p.value,
                    dadf.gdp@test$p.value,    dadf.crt@test$p.value,    
                    dadf.unemp@test$p.value,  dadf.m@test$p.value,
                    dadf.export@test$p.value, dadf.import@test$p.value,   
                    dadf.aord@test$p.value,   dadf.exr@test$p.value),4),
          
          "conclusion" 
          = c("lcpi~I(n)",     "infexp~I(0)", 
              "lgdp~I(1)",     "crt~I(1)",    
              "unemp~I(1)",    "lm~I(n)", 
              "lexport~I(1)",  "limport~I(1)", 
              "laord~I(1)",    "exr~I(1)"
          ),
  )

kable(Unit_Root_Test_table, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))

```

Consider that differencing might eliminate some of the long-term information (like trends) within the data, and considering the Minnesota prior method can be effective with nonstationary data, we have opted apply not differencing to any of the data sets.

```{r Diff Data, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false

# all_data         = na.omit(diff(all_data))

```

After logarithmic transformation of part of the data, the line plot of the dataset is as shown below (Figure 4). The dataset still retains some trend characteristics. For example, almost all variables showed significant changes before and after COVID (the x-aisx is 120).

```{r Data Plot 2, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 4 Line plot of adjusted data"

## Line plot for after transformation data
par(mfcol = c(5, 2), mar = c(2, 2, 2, 2))

for (i in 1:10) {
  ts.plot(all_data[, i], main = colnames(all_data)[i], 
          ylab = "", xlab = "", col = green3)
}
```

### Importance of Key Variables Analysis

```{r Compute inf, fig.align='center',fig.pos='H'}
cpi_lag4 = lag(cpi_data, 4)
inf_data = ((cpi_data / cpi_lag4)-1)*100
inf_data = na.omit(inf_data)
```

```{r Combine Key Data, fig.align='center',fig.pos='H'}
key_data           = na.omit(merge(inf_data, infexp_data, crt_data))
colnames(key_data) = c("inf_data", "infexp_data", "crt_data")
```

The model selects ten variables related to Australian inflation. Among the variables most directly related to inflation is inflation and inflation target. Meanwhile, because we use CPI as key variable, we first need to calculate quarterly inflation through CPI. Below we focus on analyzing these two variables:

-   The actual and expected inflation rates appear to move in tandem over much of the time period, suggesting that expectations may be influenced by current and past inflation rates, or vice versa. At the same time, we see that people's expectations for inflation are relatively conservative. For example, during the COVID, the expected inflation was around 5%, but the actual inflation later surged to about 7.8%.

-   Regarding the specific inflation zone of 2-3%, it appears that both the expected and actual inflation rates oscillate into this range periodically. However, it seems that starting just before 2020, there's a pronounced peak where expected inflation sharply rises above the actual inflation rate, which then converges back into the 2-3% range shortly after (although actual inflation rate still need some time). So we can foresee that inflation will indeed fall back to the 2-3% range in the near future.

```{r Plot inf infexp, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 5 Key data plot: Inflation rate vs Expected inflation rate"

data_df = fortify.zoo(key_data)

par(mfcol = c(1, 1), mar = c(4, 4, 2, 2) +2)

plot(data_df$Index, data_df$inf_data, type = "l", col = blue1,
     xlab = "Date", ylab = "Rate (%)", ylim = c(0, 11))

rect(xleft = min(data_df$Index), xright = max(data_df$Index),
     ybottom = 2, ytop = 3, col = green5, border = NA)  

lines(data_df$Index, data_df$inf_data, col = blue3)
lines(data_df$Index, data_df$infexp_data, col = green2)

abline(h = 0, col = green1, lty = 1) 


legend("topright", legend = c("Inflation Rate", "Expected Inflation Rate"),
       col = c(blue3, green2), lty = 1, cex = 0.6)
title(main = "Inflation Rate vs Expected Inflation Rate")

```

## Model and Hypothesis

Regarding model selection, the VAR model, capable of concurrently integrating the effects of multiple economic indicators on inflation such as GDP, exchange rates, interest rates, unemployment rates, and other variables, offers coherent and reliable results by analyzing the time series of these variables for data analysis and forecasting [@stock2001].

Based on the above analysis, all 10 variables play an important role in the fitting of the model. Use VAR(p) model for modeling, where N=10, T=137.

$$
\begin{align}
y_t &= \mu_0 + A_1y_{t-1} +...+A_py_{t-p} +\epsilon_t 
\\
\epsilon_t|Y_{t-1} &\sim iid \mathcal{N}_{10}(\textbf{0}_{10}, \Sigma)
\end{align}
$$ {#eq-2}

In matrix notation:

$$
\begin{align}
Y &= X A +E 
\\
E|X &\sim \mathcal{MN}_{T\times 10}(\textbf{0},\Sigma,I_T)
\end{align} 
$$ {#eq-3}

-   $Y$ is $T\times 10$matrix of dependent variables.

-   $X$ is $T \times (1 + 10p)$ matrix of independent variables.

-   $A$ is the matrix of coefficients, which includes the constant term $\mu_0$ and the autoregressive coefficients.

$$
Y=
\begin{bmatrix}
y_{lcpi,1} & y_{infexp,1} & y_{lgdp,1} & y_{crt,1} & y_{unemp,1} & y_{lm,1} & y_{lexport,1} & y_{limport,1} & y_{laord,1} & y_{exr,1}\\
\ y_{lcpi,2} & y_{infexp,2} & y_{lgdp,2} & y_{crt,2} & y_{unemp,2} & y_{lm,2} & y_{lexport,2} & y_{limport,2} & y_{laord,2} & y_{exr,2} \\
\vdots  & \vdots  & \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots & \vdots  \\
y_{lcpi,T} & y_{infexp,T} & y_{lgdp,T} & y_{crt,T} & y_{unemp,T} & y_{lm,T} & y_{lexport,T} & y_{limport,T} & y_{laord,T} & y_{exr,T}\\
\end{bmatrix}_{T \times 10}
$$

$$
X=\begin{bmatrix}
1 & y_{lcpi,t-1} & y_{infexp,t-1} & \ldots & y_{exr,t-1} & \ldots & y_{lcpi,t-p} & y_{infexp,t-p} & \ldots &  y_{exr,t-p} \\
1 & y_{lcpi,t-2} & y_{infexp,t-2} & \ldots & y_{exr,t-2} & \ldots & y_{lcpi,t-p-1} & y_{infexp,t-p-1} & \ldots &  y_{exr,t-p-1} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \ddots & \vdots & \vdots &  \ddots & \vdots \\
1 & y_{lcpi,1} & y_{infexp,1} & \ldots & y_{exr,1} & \ldots & y_{lcpi,1-p} & y_{infexp,1-p} & \ldots &  y_{exr,1-p}\\
\end{bmatrix}_{T\times (1+10p)}
$$

$$
A = \begin{bmatrix}
\mu_{lcpi} & \mu_{infexp} & \ldots & \mu_{lexr} \\
A_{1,lcpi}^{(1)} & A_{1,infexp}^{(1)} & \ldots & A_{1,exr}^{(1)} \\
A_{2,lpci}^{(1)} & A_{2,infexp}^{(1)} & \ldots & A_{2,exr}^{(1)} \\
\vdots  & \vdots  & \ddots & \vdots  \\
A_{10,lcpi}^{(1)} & A_{10, infexp}^{(1)} & \ldots & A_{10,exr}^{(1)} \\
\vdots  & \vdots  & \ddots & \vdots  \\
A_{1,lcpi}^{(p)} & A_{1,infexp}^{(p)} & \ldots & A_{1,exr}^{(p)} \\
A_{2,lcpi}^{(p)} & A_{2,infexp}^{(p)} & \ldots & A_{2,exr}^{(p)} \\
\vdots  & \vdots  & \ddots & \vdots  \\
A_{10,lcpi}^{(p)} & A_{10,infexp}^{(p)} & \ldots & A_{10,exr}^{(p)} \\
\end{bmatrix}_{(1 + 10p)\times 10}
$$

$$
\begin{align*}
E &= 
\begin{bmatrix}
\epsilon_{1,lcpi} & \epsilon_{1,infexp} & \ldots & \epsilon_{1,exr} \\
\epsilon_{2,lcpi} & \epsilon_{2,infexp} & \ldots & \epsilon_{2,exr} \\
\vdots & \vdots & \ddots & \vdots \\
\epsilon_{T,lcpi} & \epsilon_{T,infexp} & \ldots & \epsilon_{T,exr}
\end{bmatrix}_{T\times 10}
\\
\end{align*}
$$

### Model Application and Objective Fulfillment

Based on the model above, we will employ VAR of order p to carry out forecasts for the next two years. Given that the data is quarterly, setting the forecast horizon (h) to 8 quarters for our iterative forecasting process.

In the forecasting process, we will concentrate on determining the conditional mean and confidence interval of the projected CPI, denoted as $lcpi_{T+h|T}$. Subsequently, we'll utilize the forecasted (log) CPI values to calculate the inflation rate and assess whether it aligns with the target inflation range of 2-3%.

## Estimation Procedure and Algorithm

### Basic Model

#### Prior Distribution and Posterior Distribution Specified

The basic model based on the natural-conjugate prior distribution, which is specified as a matrix normal inverse Wishart distribution [@wozniak2016]. Minnesota prior with some stylised facts about the macroeconomic time series, since $lcpi_t$ and $lm_t$ are unit root non-stationary and other all variables are stationary, is applied to form the specifications of the prior distribution.

The estimation procedures to draw from the posterior follows the steps below:

**Step 1**: Prior distribution is presented below. We will specify $\underline{A}$, $\underline{V}$, $\underline{S}$ and $\underline{v}$.

$$
\begin{align}
p(A,\Sigma) &\propto L(A,\Sigma | Y,X) \ p(A|\Sigma) \ p(\Sigma) 
\\
p(A,\Sigma|Y,X) &= p(A|Y,X,\Sigma) \ p(\Sigma|Y,X) 
\\
\\
A|\Sigma &\sim \mathcal{MN}_{K\times N}(\underline{A}, \Sigma,\underline{V}) 
\\ 
\Sigma &\sim \mathcal{IW}_{N}(\underline{S}, \underline{\nu})
\end{align}
$$

-   $\underline{A}$ is a $K \times 10$ matrix, reflect the random walk with no drift process on the first lag of diagonal, and 0 elsewhere.

$$
\underline{A} = \left[ \underbrace{\textbf{0}_{10\times 1}}_{intercept} \quad  
\underbrace{
\begin{matrix} 
1 & \cdots  &  0 & \cdots &  0 \\
\vdots  & \ddots  & \vdots  & \ddots&  \vdots \\
0 & \cdots  & 1 & \cdots&  0 \\
\vdots  & \ddots  & \vdots  & \ddots&  \vdots \\
0 & \cdots  &  0 & \cdots &  1\\
\end{matrix}_{10\times10} \quad 
\begin{matrix}
0 & \cdots  &  0\\
\vdots  & \ddots  & \vdots  \\
0 & \cdots  & 0
\end{matrix}
_{10\times10(p-1))}}_{N\times(K-1)} 
\right]' 
$$

-   $\underline{V}$ represents the shrinking level of the specified $\underline{A}$. It's a $K$ vector diagonal matrix with the diagonal elements set to be the desired shrinking level.

$$
\underline{V} = diag\left[ \underbrace{\kappa_{2}}_{intercept} \quad \underbrace{\kappa_{1}(\textbf{p} ^{-2}\otimes I^{'}_{N})}_{A_{1} \ to \ A_{p}}  \right]
$$

-   $\underline{S}$ is $10\times10$ symmetric matrix where the diagonal represents the variances of individual variables (diagonal of $\Sigma$) and the off-diagonals are 0.

-   $\underline{v}$ is $N+2$, and it is a single values because variance is assumed to be the same for all elements of $\Sigma$.

**Step 2**: Posterior distribution shows below with the implementation of the specification in Step 1

$$
\begin{align}
p(A,\Sigma | Y,X) &= p(A|Y,X,\Sigma) \ p(\Sigma|Y,X) 
\\
 p(A|Y,X,\Sigma) &\sim \mathcal{MN}_{K\times N}(\overline{A}, \Sigma,\overline{V} ) 
\\
 p(\Sigma|Y,X) &\sim \mathcal{IW}_{N}(\overline{S}, \overline{\nu})
 \end{align}
$$

Therefore we can get:

$$
\begin{align}
\overline{A} &= \overline{V}(X'Y+\underline{V}^{-1}\underline{A}) 
\\
\overline{V} &= (X'X + \underline{V}^{-1})^{-1} 
\\ 
\overline{S} &= \underline{S}+Y'Y+\underline{A'} \ \underline{V}^{-1}\underline{A}-\overline{A'}\overline{V}^{-1}\overline{A} 
\\ 
\overline{\nu} &= T + \underline{\nu}
\end{align}
$$

**Step 3**: As $\overline{A}$, $\overline{V}$, $\overline{S}$ and $\overline{v}$ are specified:

To obtain a sample of $S$ draws from the posterior distribution:

1.  Sample $S$ draws from the marginal posterior distribution for $\Sigma^{(s)}$ given by $\mathcal{IW}_{10}(\overline{S}, \overline{v})$, and collect the draws in $\{\Sigma^{(s)}\}_{s=1}^{S}$

2.  For each draw $\Sigma^{(s)}$ for $s=1,\dots,S$ sample the corresponding draw of $A^{(s)}$ from $\mathcal{MN}_{K\times 10}(\overline{A}, \Sigma^{(s)},\overline{V} )$

Output is the sample draws from the joint posterior distribution $\left\{ {A^{(s)}, \Sigma^{(s)}} \right\}^{S}_{s=1}$.

```{r Setseed, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false

set.seed(11)

```

Function below is `posterior.draws`:

```{r Basic Model, fig.align='center',fig.pos='H'}
#| echo: true
#| message: false
#| warning: false

## Posterior sample draw function for basic model(posterior.draws)
posterior.draws       = function (S, Y, X){
  
    ## Pre-setup 
    N                 = ncol(Y) 
    t                 = N + 1
    p                 = frequency(Y)                        
    A.hat             = solve(t(X)%*%X)%*%t(X)%*%Y
    Sigma.hat         = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
    
    # Prior distribution (with Minnesota prior)

    A.prior           = matrix(0,nrow(A.hat),ncol(A.hat))
    A.prior[2:t,]     = diag(N)
    V.prior           = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
    S.prior           = diag(diag(Sigma.hat))
    nu.prior          = N+2  
    
    # Define posterior
    A.posterior       = array(NA, dim = c((1+N*p),N,S))
    Sigma.posterior   = array(NA, dim = c(N,N,S))
    
    # ------------------------posterior.draws----------------------------------
    
    # Normal-inverse Wishard posterior parameters
    V.bar.inv         = t(X)%*%X + diag(1/diag(V.prior))
    V.bar             = solve(V.bar.inv)
    A.bar             = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar            = nrow(Y) + nu.prior
    S.bar             = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv         = solve(S.bar)
    
    # Posterior draws 
    Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior   = apply(Sigma.posterior,3,solve)
    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
    L                 = t(chol(V.bar))
    
    for (s in 1:S){
      A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    }

    output            = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)
    return(output)
}
```

#### Function Proofing

Consider Bi-variate Gaussian random walk process:

$$
y_t = 
\begin{bmatrix}
y_{t,1} \\
y_{t,2}
\end{bmatrix} = 
\begin{bmatrix}
y_{t-1,1} \\
y_{t-1,2}
\end{bmatrix} + 
\begin{bmatrix}
\epsilon_{t,1} \\
\epsilon_{t,2}
\end{bmatrix} 
, where   \  \ 
\epsilon_{t,1} \sim \mathcal{N}(0,1)  \ and \ 
\epsilon_{t,2} \sim \mathcal{N}(0,1)
$$

$$ 
Y = \begin{bmatrix}
y_2' \\
y_3' \\
\vdots \\
y_n'
\end{bmatrix},
\quad
X = \begin{bmatrix}
1 \quad y_1' \\
1 \quad y_2' \\
\vdots \quad \vdots \\
1 \quad y_{n-1}'
\end{bmatrix}
$$

```{r Basic Model Function Proof, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false

e1        = cumsum(rnorm(1000, 0, sd=1))
e2         = cumsum(rnorm(1000, 0, sd=1))
e          = cbind(e1,e2)

## Define data X, Y 
Y          = ts(e[2:nrow(e),], frequency=1)
X          = matrix(1,nrow(Y),1)
X          = cbind(X,e[2:nrow(e)-1,])

S          = 10000
kappa.1    = 1                                     # shrinkage for A1 to Ap
kappa.2    = 100                                   # shrinkage for constant 
# Applying function 
posterior.sample.draws.p = posterior.draws(S=S, Y=Y, X=X)

```

After estimating a model that includes a constant term and one lag using artificial data, we observe that the posterior mean of the autoregressive and covariance matrices closely approximates an identity matrix, and the posterior mean of the constant term is nearly a vector of zeros.

-   The posterior mean of the $A$ is:

```{r Basic Model Proof A, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| tbl-cap: "Table 2 Basic Model proofing simulation for A"

basic.model.proof.A <- 
  tibble( "A" = c("Constant term", "Y1 lag","Y2 lag"),
          "Simulation Parameter Y1" 
          = c(round(mean(posterior.sample.draws.p[["A.posterior"]][1,1,]),4),
              round(mean(posterior.sample.draws.p[["A.posterior"]][2,1,]),4),
              round(mean(posterior.sample.draws.p[["A.posterior"]][3,1,]),4)),
           "Simulation Parameter Y2" 
          = c(round(mean(posterior.sample.draws.p[["A.posterior"]][1,2,]),4),
              round(mean(posterior.sample.draws.p[["A.posterior"]][2,2,]),4),
              round(mean(posterior.sample.draws.p[["A.posterior"]][3,2,]),4))

  )

kable(basic.model.proof.A, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))

```

-   The posterior mean of the $\Sigma$ is:

```{r Basic Model Proof Sigma, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| tbl-cap: "Table 3 Basic Model proofing simulation for Sigma"

basic.model.proof.Sigma <-
  tibble( "Sigma" = c("Y1 lag","Y2 lag"),
          "Simulation Parameter Y1" 
          = c(round(mean(posterior.sample.draws.p[["Sigma.posterior"]][1,1,]),4),
              round(mean(posterior.sample.draws.p[["Sigma.posterior"]][2,1,]),4)),
           "Simulation Parameter Y2" 
          = c(round(mean(posterior.sample.draws.p[["Sigma.posterior"]][1,2,]),4),
              round(mean(posterior.sample.draws.p[["Sigma.posterior"]][2,2,]),4))

  )

kable(basic.model.proof.Sigma, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))

```

### Model Extension

The model extension base on a hierarchical model. The gamma distribution on the Minnesota shrinkage parameter lambda $\lambda$ is introduced to adjust $V$.

#### Prior Distribution and Posterier Distribution Specified

**Step 1**: Prior distribution is presented below. We will specify $\underline{A}$, $\underline{V}$, $\underline{S}$, $\underline{\nu}$ by Minnesota shrinkage parameter lambda $\lambda$.

$$
\begin{align}
p(A,\Sigma, \lambda |Y,X) &\propto L(A,\Sigma | Y,X) \ p(A,\Sigma, \lambda) \\
&\propto L(A,\Sigma | Y,X)  \ p(A |\Sigma, \lambda) \ p(\Sigma) \ p(\lambda)
\end{align}
$$

where, each $p(A |\Sigma, k)$, $p(\Sigma)$, $p(k)$ is specified below:

$$
\begin{align}
A |\Sigma, \lambda &\sim \mathcal{MN}_{K\times N}(\underline{A}, \Sigma, \lambda\underline{V}) \\
\Sigma &\sim \mathcal{IW}_{N}(\underline{S},\underline{\nu}) \\
\lambda &\sim \mathcal{Gamma}(\underline{k_{\lambda}}, \underline{\theta_{\lambda}} ) \\ 
\end{align}
$$

The prior distribution is shown in the same way as the Step 1 of basic model.

**Step 2**: Posterior distribution shows below with the implementation of the specification in Step 1 by multiply $L(A,\Sigma | Y,X) \ p(A |\Sigma, \lambda) \ p(\Sigma) \ p(\lambda)$

The probability density function of Gamma distribution is:

$$
f(x;k,\theta) = \frac{x^{k-1}e^{-\frac{x}{\theta}}}{\theta^k \ \Gamma(k)} \quad  \quad   for  \ x>0, and \  k,\theta >0
$$

The kernel of Gamma distribution is:

$$
p(x|k,\theta) \propto x^{k-1}e^{-\frac{x}{\theta}}
$$

Therefore all kernel shows as follow:

$$
\begin{align}
L(A,\Sigma | Y,X) &= det(\Sigma)^{-\frac{T}{2}}\times exp\left\{ -\frac{1}{2} \mathrm{tr}\left[ \Sigma^{-1}(A-\widehat{A})'X'X (A-\widehat{A})\right]\right\} \times exp\left\{ -\frac{1}{2} \mathrm{tr}\left[ \Sigma^{-1}(Y-X\widehat{A})'(Y-X\widehat{A})\right]\right\} \\
p(A |\Sigma, \lambda) &= det(\Sigma)^{-\frac{K}{2}}det(\lambda \underline{V} )^{-\frac{N}{2}}exp\left\{ -\frac{1}{2} \mathrm{tr}\left[ \Sigma^{-1} (A-\underline{A})' (\lambda\underline{V})^{-1} (A-\underline{A})\right]\right\} \\ 
p(\Sigma) &= det(\Sigma)^{\frac{\underline{\nu}+N+1}{2}}exp\left\{ -\frac{1}{2} \mathrm{tr}\left[ \Sigma^{-1} \underline{S}\right]\right\}  \\
p(\lambda) &= \lambda ^{\underline{k_{\lambda}}-1}e^{-\frac{\lambda}{\underline{\theta_{\lambda}}}}
\end{align}
$$

-   The kernel of the fully conditional posterior distribution of $A$ and $\Sigma$:

$$
\begin{align}
p(A,\Sigma |Y,X, \lambda) &\propto L(Y,X|A,\Sigma)p(A |\Sigma, \lambda)p(\Sigma) \\
&\propto \det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}} \\
&\times \exp\left\{ -\frac{1}{2} \mathrm{tr}[\Sigma^{-1}[(A-\overline{A})'\overline{V}^{-1}(A-\overline{A})+\underline{S}+Y'Y+\underline{A}'(\lambda\underline{V})^{-1}\underline{A}-\overline{A}'\overline{V}^{-1}\overline{A}]  ] \right\} \\
\end{align}
$$

Now, we can get:

$$
\begin{align}
\overline{A} &= \overline{V}(X'Y+(\lambda\underline{V})^{-1}\underline{A}) \\
\overline{V} &= (X'X + (\lambda\underline{V})^{-1})^{-1}\\
\overline{S} &= \underline{S}+Y'Y+\underline{A}'(\lambda\underline{V})^{-1}\underline{A}-\overline{A}'\overline{V}^{-1}\overline{A} \\
\overline{\nu} &= T+\underline{\nu}
\end{align}
$$

-   The kernel of the fully conditional posterior distribution of $\lambda$:

$$
\begin{align}
p(\lambda |Y,X,A,\Sigma ) &\propto L(Y,X|A,\Sigma)p(A,\Sigma, \lambda)\\
&\propto L(Y,X|A,\Sigma)p(A |\Sigma, \lambda)p(\Sigma)p(\lambda) \\
&\propto p(A |\Sigma, \lambda)p(\lambda) 
\end{align}
$$

$$
\begin{align}
p(\lambda |Y,X,A,\Sigma ) 
&\propto p(A |\Sigma, \lambda)p(\lambda) \\
&\propto 
det(\lambda \underline{V} )^{-\frac{N}{2}}exp\left\{ -\frac{1}{2} \mathrm{tr}\left[ \Sigma^{-1} (A-\underline{A})' (\lambda\underline{V})^{-1} (A-\underline{A})\right]\right\}  \times \lambda ^{\underline{k_{\lambda}}-1}e^{-\frac{\lambda}{\underline{\theta_{\lambda}}}} \\ 
&\propto 
det(\underline{V} )^{-\frac{N}{2}} \lambda^{-\frac{KN}{2}} exp\left\{ -\frac{1}{2} \mathrm{tr}\left[ \Sigma^{-1} (A-\underline{A})' (\lambda\underline{V})^{-1} (A-\underline{A})\right]\right\}  \times \lambda ^{\underline{k_{\lambda}}-1}exp \left\{ -\frac{\lambda}{\underline{\theta_{\lambda}}} \right\} \\
&\propto 
 \lambda^{-\frac{KN}{2} + \underline{k_{\lambda}}-1 } exp\left\{ -\frac{1}{2} \mathrm{tr}\left[ \Sigma^{-1} (A-\underline{A})' (\lambda\underline{V})^{-1} (A-\underline{A})\right]-\frac{\lambda}{\underline{\theta_{\lambda}}} \right\} \\
&\propto 
 \lambda^{-\frac{KN}{2} + \underline{k_{\lambda}}-1 } exp\left\{ -\frac{\mathrm{tr}\left[ \Sigma^{-1} (A-\underline{A})' (\underline{V})^{-1} (A-\underline{A})\right]}{2\lambda} 
-\frac{\lambda}{\underline{\theta_{\lambda}}} \right\}\\
&\propto 
\lambda^{-\frac{KN}{2} + \underline{k_{\lambda}}-1 } 
exp\left\{ \frac{- \left[ \frac{1}{\lambda} \mathrm{tr}  \left[ \Sigma^{-1} (A-\underline{A})' (\underline{V})^{-1} (A-\underline{A})\right]  + \frac{\lambda}{ \frac{1}{2} \underline{\theta_{\lambda}}}\right]}{2} 
\right\}\\
\end{align}
$$

As we can figure out the kernel follows generalized inverse Gaussian distribution (GIG):

$$
f(x;a,b,p)=\frac{(a/b)^{p/2}}{2K_p(\sqrt{ab})}x^{(p-1)}e^{-(ax+b/x)/2}, \quad \quad    x>0
$$

The kernel of GIG:

$$
p(x|a,b,p) \propto x^{p-1}exp \left\{\frac{-(ax+\frac{b}{x})}{2} \right\}
$$

Hence, the full-conditional posterior distribution of $\lambda$ follows a Generalised Inverse Gaussian distribution.

$$
\lambda|Y,X,A,\Sigma \sim \mathcal{GIG}(\overline{a},\overline{b},\overline{p})
$$

Therefore we can get:

$$
\begin{align}
\overline{a} &= \frac{2}{\underline{\theta_{\lambda}} }
\\
\overline{b} &= \mathrm{tr}  \left[ \Sigma^{-1} (A-\underline{A})' (\underline{V})^{-1} (A-\underline{A})\right] 
\\ 
\overline{p} &= -\frac{KN}{2} + \underline{k_{\lambda}}
\end{align}
$$

**Step 3**: As $\overline{A}$, $\overline{V}$, $\overline{S}$, $\overline{v}$, $\overline{a}$, $\overline{b}$, $\overline{p}$ are specified:

Initialize $\lambda$ at $\lambda^{(0)}$.

At each iteration $s$:

1.  Draw random matrices for $A^{(s)}$ and $\Sigma^{(s)}$ from $p(A,\Sigma|Y,X,\lambda^{(s-1)})$.

2.  Draw a random number for $\lambda^{(s)}$ from $p(\lambda |Y,X,A^{(s)},\Sigma^{(s)})$.

Repeat 1 and 2 $S_{1} + S_{2}$ times.

Discard the first $S_{1}$ draws that allows the algorithm to converge to the stationary posterior distribution.

Output is the sample draws from the joint posterior distribution $\left\{ {A^{(s)}, \Sigma^{(s)}, \lambda^{(s)}} \right\}^{S_{1}+S_{2}}_{s=S_{1}+1}$.

Function below is `posterior.draws.exten`:

```{r Exten Model, fig.align='center',fig.pos='H'}
#| echo: true
#| message: false
#| warning: false

## Posterior sample draw function for extended model(posterior.draws.exten)
posterior.draws.exten = function (total_S, Y, X){
  
  ## Pre-setup 
  N             = ncol(Y) 
  t             = N + 1
  p             = frequency(Y)                        
  A.hat         = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  K             = ncol(X)
  
  
  # Prior distribution (with Minnesota prior)

  A.prior           = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:t,]     = diag(N)
  V.prior           = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior           = diag(diag(Sigma.hat))
  nu.prior          = N+2  
  
  # Define posterior
  A.posterior       = array(NA, dim = c((1+N*p),N,S1+S2))
  Sigma.posterior   = array(NA, dim = c(N,N,S1+S2))
  lambda.posterior  = matrix(NA, S1+S2, 1)
  
  # initial value of lambda
  lambda.posterior[1] = 10                            # set lambda0 

  # ----------------------posterior.draws.exten------------------------------
  
  for (s in 1:total_S){
      # NIW posterior parameters
      V.bar.inv              = t(X)%*%X + diag(1/diag(lambda.posterior[s]* V.prior)) 
      V.bar                  = solve(V.bar.inv)
      A.bar                  = V.bar%*%(t(X)%*%Y + diag(1/diag(lambda.posterior[s]* V.prior))%*%A.prior)
      nu.bar                 = nrow(Y) + nu.prior
      S.bar                  = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(lambda.posterior[s]* V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
      S.bar.inv              = solve(S.bar)
    
      # posterior draws for A and Sigma
      Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
      Sigma.posterior.draw   = apply(Sigma.posterior.IW,3,solve)
      Sigma.posterior[,,s]   = Sigma.posterior.draw
      A.posterior[,,s]       = array(rnorm(prod(c(dim(A.bar),1))),c(dim(A.bar),1))
      L                      = t(chol(V.bar))
      A.posterior[,,s]       = A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
      
      
      # Update parameters for lambda posterior
      p                      = lambda.priors$k - (K * N)/2              # N=10
      diff_A                 = A.posterior[,,s] - A.prior
      product                = t(diff_A) %*% solve(V.prior) %*% diff_A
      b                      = sum(diag(solve(Sigma.posterior[,,s] %*% product)))
      a                      = 2 / lambda.priors$theta
      
      # Draw next period value for lambda from GIG distribution
      if (s!=total_S){
        lambda.posterior[s+1] = GIGrvg::rgig(n=1, lambda = p, chi = b, psi = a)
      }
    }
    
      output                 = list(A.posterior.exten = A.posterior[,,(S1+1):S2], 
                                    Sigma.posterior.exten = Sigma.posterior[,,(S1+1):S2], 
                                    lambda.posterior.exten = lambda.posterior[(S1+1):S2,])
  
      return(output)
  }
```

#### Function Proofing

```{r Extend Model Function Proof, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false

# setup 
kappa.1             = 1                             # shrinkage for A1 to Ap
kappa.2             = 100                           # shrinkage for constant   


# Prior Gamma distribution: k, theta
lambda.priors       = list(k = 1, theta = .1)

S1                  = 500                           # determine the burn-in draws
S2                  = 9500                          # number of draws from the final simulation
total_S             = S1+S2

# Applying function 
posterior.extend.draws.p = posterior.draws.exten(total_S = total_S, Y = Y, X = X)
```

After fitting a model that includes a constant term and one lag with artificial data, just like the basic model, the extend model also shows that the posterior mean of both the autoregressive and covariance matrices closely identity matrix, and the posterior mean of the constant term is almost a vector of zeros.

-   The posterior mean of the $A$ is:

```{r Extend Model Proof A, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| tbl-cap: "Table 4 Extend Model proofing simulation for A"

extend.model.proof.A <-
  tibble( "A" = c("Constant term", "Y1 lag","Y2 lag"),
          "Simulation Parameter Y1" 
          = c(round(mean(posterior.extend.draws.p[["A.posterior.exten"]][1,1,]),4),
              round(mean(posterior.extend.draws.p[["A.posterior.exten"]][2,1,]),4),
              round(mean(posterior.extend.draws.p[["A.posterior.exten"]][3,1,]),4)),
           "Simulation Parameter Y2" 
          = c(round(mean(posterior.extend.draws.p[["A.posterior.exten"]][1,2,]),4),
              round(mean(posterior.extend.draws.p[["A.posterior.exten"]][2,2,]),4),
              round(mean(posterior.extend.draws.p[["A.posterior.exten"]][3,2,]),4))
  )

kable(extend.model.proof.A, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))

```

-   The posterior mean of the $\Sigma$ is:

```{r Extend Model Proof Sigma, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| tbl-cap: "Table 5 Extend Model proofing simulation for Sigma"

extend.model.proof.Sigma <-
  tibble( "Sigma" = c("Y1 lag","Y2 lag"),
          "Simulation Parameter Y1" 
          = c(round(mean(posterior.extend.draws.p[["Sigma.posterior.exten"]][1,1,]),4),
              round(mean(posterior.extend.draws.p[["Sigma.posterior.exten"]][2,1,]),4)),
           "Simulation Parameter Y2" 
          = c(round(mean(posterior.extend.draws.p[["Sigma.posterior.exten"]][1,2,]),4),
              round(mean(posterior.extend.draws.p[["Sigma.posterior.exten"]][2,2,]),4))
  )

kable(extend.model.proof.Sigma, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))

```

## Empirical Analysis - Model Applying and Forecasing

### Basic Model

```{r Stat Setup for applying, fig.align='center',fig.pos='H'}

## Create Y and X
y             = ts(all_data[,1:ncol(all_data)])     # 10col, 135row
Y             = ts(y[5:nrow(y),], frequency=4)      # 10col, 131row
X             = matrix(1,nrow(Y),1)
for (i in 1:frequency(Y)){
  X           = cbind(X,y[5:nrow(y)-i,])            # 10*4+1=41col, 131row
}
 
```

```{r Function Applying, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false

## Set up 
S                      = 10000
kappa.1                = 1               # shrinkage for A1 to Ap
kappa.2                = 100             # shrinkage for constant 

## Function Applying
posterior.sample.draws = posterior.draws(S = S, Y = Y, X = X) 

A.posterior.simu       = posterior.sample.draws$A.posterior
Sigma.posterior.simu   = posterior.sample.draws$Sigma.posterior

```

```{r Basic Model Forecasting, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false

## Two-year ahead forecasting h = 8 and with p = 4

# set up
h                      = 8
N                      = ncol(Y) 
Y.h                    = array(NA,c(h,N,S))
p                      = 4

# create sampling predictive density function 
sampling.predictive.density.fun = function(A.posterior.simu, Sigma.posterior.simu, S = S) 
{
  for (s in 1:S){
    A.posterior.draw     = A.posterior.simu[,,s]
    Sigma.posterior.draw = Sigma.posterior.simu[,,s]
      x.Ti               = Y[(nrow(Y)-p+1):nrow(Y),]
      x.Ti               = x.Ti[p:1,]
    for (i in 1:h){
      x.T                = c(1,as.vector(t(x.Ti)))
      Y.f                = rmvnorm(1, mean = x.T%*%A.posterior.draw, sigma=Sigma.posterior.draw)
        x.Ti             = rbind(Y.f,x.Ti[1:(p-1),])
      Y.h[i,,s]          = Y.f[1:N]
    }
  }
    return (Y.h)
}

# sampling predictive density
Y.h       = sampling.predictive.density.fun(A.posterior.simu = A.posterior.simu,
                                            Sigma.posterior.simu = Sigma.posterior.simu,
                                            S = S) 

```

Figure 6 and 7 presents a 3D and 2D visualization of the density intervals for the log CPI and inflation expectations. Inflation expectations show some fluctuations, while the log CPI exhibits a stable increase. as indicated by the narrow bounded confidence intervals (light blue for CPI and light green for inflation expectations). The varying heights of the intervals reflect the level of prediction certainty; as we project further into the future, the intervals become wider and more dispersed due to increased uncertainty.

```{r 3d forecasting graph on basic model, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 6 3D forecasting graph on basic model"


# create 3D forecasting graph function 

threed.graph = function(Y.h){

  par(mfcol = c(1, 2), mar=c(2,2,2,2)-0.1)
  
  # Log CPI forecasting 
  limits.lcpi      = range(Y.h[,1,])
  point.lcpi.f     = apply(Y.h[,1,],1,mean)
  interval.lcpi.f  = apply(Y.h[,1,],1,hdi,credMass=0.90)
  
  x                = seq(from=limits.lcpi[1], to=limits.lcpi[2], length.out=100)
  z                = matrix(NA,h,99)
  for (i in 1:h){
    z[i,]          = hist(Y.h[i,1,], breaks=x, plot=FALSE)$density
  }
  x                = hist(Y.h[i,1,], breaks=x, plot=FALSE)$mids
  yy               = 1:h
  z                = t(z)
  
  f4               = persp3D(x=x, y=yy, z=z, phi=15.5, theta=180, 
                             xlab="\ncpi[t+h|t]", ylab="h", zlab="\npredictive densities of cpi",
                             shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1,
                             col=NA,plot=FALSE)
  perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=15.5, theta=180, 
            xlab="\nlog.cpi[t+h|t]", ylab="h", zlab="\npredictive densities of cpi", 
            ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)
  
  polygon3D(x=c(interval.lcpi.f[1,],interval.lcpi.f[2,h:1]), 
            y=c(1:h,h:1), 
            z=rep(0,2*h), 
            col = blue2, NAcol = blue5, border = NA, add = TRUE, plot = TRUE)
  
  for (i in 1:h){
    f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
    lines(f4.l, lwd=1, col=blue3)
  }
  f4.l1 = trans3d(x=point.lcpi.f, y=yy, z=0, pmat=f4)
  lines(f4.l1, lwd=1.5, col=green1)
  
  # Expected inflation rate forecasting 
  limits.infexp      = range(Y.h[,2,])
  point.infexp.f     = apply(Y.h[,2,],1,mean)
  
  interval.infexp.f  = apply(Y.h[,2,],1,hdi,credMass=0.90)
  
  x                = seq(from=limits.infexp[1], to=limits.infexp[2], length.out=100)
  z                = matrix(NA,h,99)
  for (i in 1:h){
    z[i,]          = hist(Y.h[i,2,], breaks=x, plot=FALSE)$density
  }
  x                = hist(Y.h[i,2,], breaks=x, plot=FALSE)$mids
  yy               = 1:h
  z                = t(z)
  
  f4               = persp3D(x=x, y=yy, z=z, phi=15.5, theta=180, 
                             xlab="\ninfexp[t+h|t]", ylab="h", zlab="\npredictive densities of infexp",
                             shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1,
                             col=NA,plot=FALSE)
  perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=15.5, theta=180, 
            xlab="\ninfexp[t+h|t]", ylab="h", zlab="\npredictive densities of infexp", 
            ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)
  
  polygon3D(x=c(interval.infexp.f[1,],interval.infexp.f[2,h:1]), 
            y=c(1:h,h:1), 
            z=rep(0,2*h), 
            col = green5, NAcol = green2, border = NA, add = TRUE, plot = TRUE)
  
  for (i in 1:h){
    f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
    lines(f4.l, lwd=1, col=green6)
  }
  f4.l1 = trans3d(x=point.infexp.f, y=yy, z=0, pmat=f4)
  lines(f4.l1, lwd=1.5, col=green1)

}

threed.basic.graph = threed.graph(Y.h = Y.h)
```

```{r 2d forecasting graph on basic model, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 7 2D forecasting graph on basic model"

twod.graph = function(Y.h){

  point.lcpi.f         = apply(Y.h[,1,],1,mean)
  interval.lcpi.f      = apply(Y.h[,1,],1,hdi,credMass=0.90)
  lcpi.range           = range(y[,1],interval.lcpi.f)
  
  point.infexp.f       = apply(Y.h[,2,],1,mean)
  interval.infexp.f    = apply(Y.h[,2,],1,hdi,credMass=0.90)
  infexp.range         = range(y[,2],interval.infexp.f)
  
  par(mfcol = c(2,1), mar=c(2,2,2,2)-0.1)
  plot(1:(length(y[,1])+h),c(y[,1],point.lcpi.f), type="l", ylim=c(3.9, 5.1), axes=FALSE, xlab="lcpi", ylab="", lwd=2, col=blue1)
  axis(1,c(1,41,81,121,nrow(y),nrow(y)+h),c("1990","2000","2010","2020","",""), col=green1)
  
  axis(2, at = pretty(lcpi.range), labels = pretty(lcpi.range), col = green1)
  abline(v=137, col=green1)
  polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[9:1]),
          c(y[137,1],interval.lcpi.f[1,],interval.lcpi.f[2,8:1],y[137,1]),
          col=blue4.shade1, border=blue4.shade1)
  
  plot(1:(length(y[,2])+h),c(y[,2],point.infexp.f), type="l", ylim=c(0,5), axes=FALSE, xlab="infexp", ylab="", lwd=2, col=green6)
  axis(1,c(1,41,81,121,nrow(y),nrow(y)+h),c("1990","2000","2010","2020","",""), col=green1)
  
  axis(2, at = pretty(infexp.range), labels = pretty(infexp.range), col = green1)
  
  abline(v=137, col=green1)
  polygon(c(length(y[,2]):(length(y[,2])+h),(length(y[,2]):(length(y[,2])+h))[9:1]),
          c(y[137,2],interval.infexp.f[1,],interval.infexp.f[2,8:1],y[137,2]),
          col=green3.shade1, border=green3.shade1)

}

twod.basic.graph = twod.graph(Y.h = Y.h)
```

```{r Basic Model forecast data transform, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
# transform cpi into inflation rate, collect with it with infexp and cash rate

dates = as.Date(c("2024-06-30", "2024-09-30", "2024-12-31",
                   "2025-03-31", "2025-06-30", "2025-09-30", "2025-12-31", "2026-03-31" ))

# create data transform function
data.trans = function(Y.h){
  ## transform cpi into inflation rate
  point.lcpi.h            = ts(apply(Y.h[,1,],1,mean))
  point.lcpi.h = xts(point.lcpi.h, order.by = dates)
  
  lcpi.data.h             = c(lcpi_data["/2024-03-31"], point.lcpi.h)
  cpi.data.h              = exp(lcpi.data.h)
  
  cpi.lag4.h = lag(cpi.data.h, 4)
  inf.data.h = ((cpi.data.h / cpi.lag4.h)-1)*100
  inf.data.h = na.omit(inf.data.h)
  
  ## inflation expectation
  
  point.infexp.h            = ts(round(apply(Y.h[,2,],1,mean),6))
  point.infexp.h            = xts(point.infexp.h, order.by = dates)
  infexp.data.h             = c(infexp_data["/2024-03-31"], point.infexp.h )
  
  key_data.h           = na.omit(merge(inf.data.h, infexp.data.h))
  colnames(key_data.h) = c("inf_data.h", "infexp_data.h")
  
  return(key_data.h)
}

key.data.h = data.trans(Y.h = Y.h)
```

After an in-depth examination of key variables and converting the log CPI to CPI to calculate the inflation rate for the next eight periods (Figure 8), we found contrary results. Both the inflation rate and inflation expectations show a downward trend. Although they will remain within the RBA's target range for a while, they are expected to fall below 2% after 2025. As for people's inflation expectations, the forecast indicates a fluctuating decline.

```{r Basic Model Key Data Plot, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 8 Basic Model key data plot"

key.data.plot = function(key.data.h)
{
  num.h      = nrow(key.data.h)
  num        = num.h-h
  data_df    = fortify.zoo(key.data.h)
  
  par(mfcol = c(1, 1), mar = c(4, 4, 2, 2) +2)
  
  plot(data_df$Index, data_df$inf_data.h, type = "l", col = blue1,
       xlab = "Date", ylab = "Rate (%)", ylim = c(0, 11))
  
  rect(xleft = min(data_df$Index), xright = max(data_df$Index),
       ybottom = 2, ytop = 3, col = green5, border = NA)  
  
  lines(data_df$Index[1:num], data_df$inf_data.h[1:num], col = blue1)
  lines(data_df$Index[1:num], data_df$infexp_data.h[1:num], col = green2)
  
  lines(data_df$Index[num:length(data_df$Index)], data_df$inf_data.h[num:length(data_df$Index)], col = blue4)
  lines(data_df$Index[num:length(data_df$Index)], data_df$infexp_data.h[num:length(data_df$Index)], col = green3)
  
  abline(v = data_df$Index[139], col = green1, lty = 2) 
  abline(h = 0, col = green1, lty = 1) 
  
  legend("topright", legend = c("Inflation Rate", "Expected Inflation Rate", "Inflation Rate Forecast", "Expected Inflation Rate Forecast", "2024-03-31"),
         col = c(blue1, green2, blue4, green3, green1), lty = 1, cex = 0.6)
  # title(main = "Inflation Rate vs Expected Inflation Rate Forecasing by Basic Model")
}

key.data.basic.graph = key.data.plot(key.data.h = key.data.h)
```

### Extension Model

```{r Extension Function Applying, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false

# setup 
S1                  = 500                           # determine the burn-in draws
S2                  = 9500                          # number of draws from the final simulation
total_S             = S1 + S2

kappa.1             = 1                             # shrinkage for A1 to Ap
kappa.2             = 100                           # shrinkage for constant   

# Prior Gamma distribution: k, theta
lambda.priors       = list(k = 1, theta = .1)


## Function Applying
posterior.extend.draws     = posterior.draws.exten(total_S = total_S, Y=Y, X=X)
A.posterior.ext.simu       = posterior.extend.draws[["A.posterior.exten"]]
Sigma.posterior.ext.simu   = posterior.extend.draws[["Sigma.posterior.exten"]]

```

```{r Extension Function Foracasting, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
## two-year ahead forecasting h = 8 and p = 4
# set up
h                 = 8
S                 = 9000
N                 = ncol(Y) 
Y.h.ext           = array(NA,c(h,N,S))
p                 = 4


# sampling predictive density

Y.h.ext       = sampling.predictive.density.fun(A.posterior.simu = A.posterior.ext.simu,
                                                Sigma.posterior.simu = Sigma.posterior.ext.simu,
                                                S = S) 
```

Figure 9 and 10 presents a 3D and 2D visualization of the density intervals for the log CPI and inflation expectations calculated by the extension model. The results are generally similar to those of the basic model, with the only difference being that the log CPI shows a downward trend instead of an upward one.

```{r 3d forecasting graph on extension model, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 9 3d forecasting graph on extension model"

threed.ext.graph = threed.graph(Y.h = Y.h.ext)
```

```{r 2d forecasting graph on extension model, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 10 2d forecasting graph on extension model"

twod.ext.graph = twod.graph(Y.h = Y.h.ext)

```

```{r Extend Model forecast data transform, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false

# transform cpi into inflation rate, collect with it with infexp and cash rate

key.data.h.ext = data.trans(Y.h = Y.h.ext)

```

For the analysis of key data, we can see that the extension model presents a more aggressive outlook compared to the basic model (Figure 11). The expected inflation rate continues to decline over the next eight quarters, reaching negative values by the end of 2024 (deflation). People's inflation expectations are also volatile, fluctuating within the 0%-2% range.

```{r Extend Model Key Data Plot, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 11 Extend Model key data plot"

key.data.ext.graph = key.data.plot(key.data.h.ext)
```

Until here, we realized that the extended model is more radical, and we decided to rethink the prior settings in the extended model and compare: In the extended model, consider the coefficient $\lambda$(follow gamma distribution.)

```{r gamma dist, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 12 Gamma distribution"

lambda.priors = list(
  k = c(1, 100, 1, 100),
  theta = c(0.1, 0.1, 0.5, 0.5)
)

x <- seq(0, 15, length.out = 1000)

lambda <- data.frame()
for (i in 1:4) {
  k <- lambda.priors$k[i]
  theta <- lambda.priors$theta[i]
  y <- dgamma(x, shape = k, scale = theta)
  lambda <- rbind(lambda, data.frame(x = x, y = y, k = k, theta = theta))
}

ggplot(lambda, aes(x = x, y = y, color = interaction(k, theta))) +
  geom_line() +
  labs(title = "Gamma Distribution",
       x = "x",
       y = "Density",
       color = "k, theta") +
  theme_minimal()
color = scales::hue_pal()(4)

```

For the Gamma distribution with parameters $(k,θ)=(1,0.1)$ (red line), the density is highly concentrated near zero, making it very steep. This distribution is ideal for modeling high-frequency, short-duration events such as waiting times or time between failures. Compared to distributions with larger shape or scale parameters, this one lacks support for larger values, indicating most events occur within a very short time frame.

```{r prior test, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 13 Predictive density of lcpi with different k and theta"

# setup 
S1                = 500                            # determine the burn-in draws
S2                = 9500                           # number of draws from the final simulation
h                 = 8 
S                 = 9000 

# Prior Gamma distribution: k, theta

# set up color
colors <- c(color[1], color[2], color[3], color[4])

par(mfcol = c(2, 2), mar = c(2, 2, 2, 2))

for (i in 1:4) {
  k                        = lambda.priors$k[i]
  theta                    = lambda.priors$theta[i]
  color                    = colors[i]

  posterior.extend.draws.  = posterior.draws.exten(total_S = S1 + S2, Y = Y, X = X)
  A.posterior.ext.simu     = posterior.extend.draws[["A.posterior.exten"]]
  Sigma.posterior.ext.simu = posterior.extend.draws[["Sigma.posterior.exten"]]

  Y.h.ext                  = array(NA, c(h, N, S))

  Y.h.ext                  = sampling.predictive.density.fun(
    A.posterior.simu = A.posterior.simu,
    Sigma.posterior.simu = Sigma.posterior.simu,
    S = S) 

  limits.lcpi.ext         = range(Y.h.ext[,1,])
  point.lcpi.f.ext        = apply(Y.h.ext[,1,], 1, mean)

  interval.lcpi.f.ext     = apply(Y.h.ext[,1,], 1, hdi, credMass = 0.90)

  x <- seq(from = limits.lcpi.ext[1], to = limits.lcpi.ext[2], length.out = 100)
  z <- matrix(NA, h, 99)
  for (i in 1:h) {
    z[i,] <- hist(Y.h.ext[i,1,], breaks = x, plot = FALSE)$density
  }
  x <- hist(Y.h.ext[i,1,], breaks = x, plot = FALSE)$mids
  yy <- 1:h
  z <- t(z)

  f4 <- persp3D(x = x, y = yy, z = z, phi=15.5, theta=180, 
                xlab = "\ncpi[t+h|t]", ylab = "h", zlab = "\npredictive densities of cpi",
                shade = NA, border = NA, ticktype = "detailed", nticks = 3, cex.lab = 1,
                col = NA, plot = FALSE)
  perspbox(x = x, y = yy, z = z, bty = "f", col.axis = "black", phi=15.5, theta=180, 
           xlab = "\nlog.cpi[t+h|t]", ylab = "h", zlab = "\npredictive densities of cpi", 
           ticktype = "detailed", nticks = 3, cex.lab = 1, col = NULL, plot = TRUE)

  polygon3D(x = c(interval.lcpi.f.ext[1,], interval.lcpi.f.ext[2,h:1]), 
            y = c(1:h, h:1), 
            z = rep(0, 2*h), 
            col = adjustcolor(color, alpha.f = 0.3), border = NA, add = TRUE, plot = TRUE)

  for (i in 1:h) {
    f4.l <- trans3d(x = x, y = yy[i], z = z[,i], pmat = f4)
    lines(f4.l, lwd = 1, col = color)
  }
  f4.l1 <- trans3d(x = point.lcpi.f.ext, y = yy, z = 0, pmat = f4)
  lines(f4.l1, lwd = 1.5, col = adjustcolor(color, alpha.f = 0.7))
}

```

The predictive density distribution remains stable despite changes in the prior distribution of the Gamma distribution in lambda, indicating that the predictive values have stabilized in the Gibbs sampler. This suggests that our forecast is robust, and resulting in a stable posterior distribution and predictive outcome. Consequently, the predictive results are primarily driven by the data rather than the prior assumptions.

## Stochastic Volatility Conditional Heteroskedasticity

Until now, we acknowledge that our model has not achieved the most optimal results. Our modeling approach has operated under the assumption of homoscedasticity. However, in real-world, such as the 2008 financial crisis and the 2020 COVID-19 pandemic, have demonstrated the significant and often abrupt fluctuations in the economy and financial markets. These events underscore the necessity of incorporating heteroscedasticity into our model to more accurately reflect the variable nature of economic conditions and market volatility.

### Basic Model with Heteroskedasticity

The model specification is as below:

$$
\begin{align}
Y &= X A +E 
\\
E|X &\sim \mathcal{MN}_{T\times 10}(\textbf{0},\Sigma,\mathrm{diag}(\sigma_{T}^{2}))
\end{align} 
$$ {#eq-4}

**Step 1**: Prior distribution is presented below. We will specify $\underline{A}$, $\underline{V}$, $\underline{S}$ and $\underline{v}$.

$$
\begin{align}
p(A,\Sigma) &\propto L(A,\Sigma | Y,X) \ p(A,\Sigma) \\
p(A,\Sigma) &\propto L(A,\Sigma | Y,X) \ p(A|\Sigma) \ p(\Sigma) 
\\
\\
A|\Sigma &\sim \mathcal{MN}_{K\times N}(\underline{A}, \Sigma,\underline{V}) 
\\ 
\Sigma &\sim \mathcal{IW}_{N}(\underline{S}, \underline{\nu})
\end{align}
$$

-   Likelihood function

$$
\begin{align} 
L\left( {A},{\Sigma}|Y,X \right) &\propto \text{det}({\Sigma})^{-\frac{T}{2}}\exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}(Y-X{A})'(Y-X{A}) \right] \right\}\\
&=\text{det}({\Sigma})^{-\frac{T}{2}}\\
&\quad\times\exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}({A}-\widehat{A})'X' \ \mathrm{diag}(\sigma_{T}^{2}) \ X({A}-\widehat{A}) \right] \right\}\\
&\quad\times \exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}(Y-X\widehat{A})' \ \mathrm{diag}(\sigma_{T}^{2}) \ (Y-X\widehat{A}) \right] \right\}
\end{align} 
$$

-   Natural-conjugate prior distribution (kernel)

$$
\begin{align} 
p\left({A},{\Sigma} \right) &\propto \text{det}({\Sigma})^{-\frac{N+K+\underline{\nu}+1}{2}}\\&\quad\times\exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}({A}-\underline{A})'\underline{V}^{-1}({A}-\underline{A}) \right] \right\}\\&\quad\times \exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}\underline{S} \right] \right\}
\end{align} 
$$

**Step 2**: Posterior distribution shows below with the implementation of the specification in Step 1

$$
\begin{align} 
p\left( {A}, {\color{purple}\Sigma}|Y,X \right) 
&\propto L({A},{\Sigma}|Y,X)p\left( {A}, {\Sigma} \right)\\
&= L({A},{\Sigma}|Y,X)p\left( {A}| {\Sigma} \right)p\left( {\Sigma} \right)
\end{align} 
$$

-   kernel:

$$
\begin{align*} 
p\left( {A},{\Sigma} |Y,X\right) 
&\propto  \text{det}({\Sigma})^{-\frac{T}{2}}\\
&\quad\times\exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}({A}-\widehat{A})'X' \ \mathrm{diag}(\sigma_{T}^{2}) \ X({A}-\widehat{A}) \right] \right\}\\
&\quad\times \exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}(Y-X\widehat{A})' \ \mathrm{diag}(\sigma_{T}^{2}) \ (Y-X\widehat{A}) \right] \right\}\\
& \quad\times\text{det}({\Sigma})^{-\frac{N+K+\underline{\nu}+1}{2}}\\
&\quad\times\exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}({A}-\underline{A})'\underline{V}^{-1}({A}-\underline{A}) \right] \right\}\\
&\quad\times \exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1}\underline{S} \right] \right\} \\
&\propto\text{det}({\Sigma})^{-\frac{\overbrace{T+\underline{\nu}}^{\overline{\nu}} +N+K++1}{2}} \\
&\quad\times\exp\left\{ -\frac{1}{2}\text{tr}\left[ {\Sigma}^{-1} 
\color{blue}{\left[({A}-\widehat{A})'X' \ \mathrm{diag}(\sigma_{T}^{2}) \ X({A}-\widehat{A}) + ({A}-\underline{A})'\underline{V}^{-1}({A}-\underline{A})  +(Y-X\widehat{A})' \ \mathrm{diag}(\sigma_{T}^{2}) \ (Y-X\widehat{A}) + \underline{S} \right]} \right] \right\}
\end{align*}
$$

Focus on the blue part:

$$
\begin{align} 
& \quad \quad({A}-\widehat{A})'X' \ \mathrm{diag}(\sigma_{T}^{2}) \ X({A}-\widehat{A}) + ({A}-\underline{A})'\underline{V}^{-1}({A}-\underline{A}) +(Y-X\widehat{A})' \ \mathrm{diag}(\sigma_{T}^{2}) \ (Y-X\widehat{A}) + \underline{S} 
\\
&= A'X'\mathrm{diag}(\sigma_{T}^{2})XA - A'X'\mathrm{diag}(\sigma_{T}^{2})X\widehat{A} -\widehat{A}'X'\mathrm{diag}(\sigma_{T}^{2})XA + \underbrace{\widehat{A}'X'\mathrm{diag}(\sigma_{T}^{2})X\widehat{A}}_{\text{cancel out 1}}  + A'\underline{V}^{-1}A -A'\underline{V}^{-1}\underline{A} - \underline{A}'\underline{V}^{-1}A +\underline{A}'\underline{V}^{-1}\underline{A}' 
\\
& \qquad + Y'\mathrm{diag}(\sigma_{T}^{2})Y - \underbrace{Y'\mathrm{diag}(\sigma_{T}^{2})X\widehat{A}}_{\text{cancel out 2}}  - \underbrace{\widehat{A}'X'\mathrm{diag}(\sigma_{T}^{2})Y}_{\text{cancel out 2} }  + \underbrace{\widehat{A}'X'\mathrm{diag}(\sigma_{T}^{2})X\widehat{A} }_{\text{cancel out 1}} +\underline{S} 
\\
&= \underbrace{A'X'\mathrm{diag}(\sigma_{T}^{2})XA}_{\text{merge 1}}  - A'X'\mathrm{diag}(\sigma_{T}^{2})X\widehat{A} -\widehat{A}'X'\mathrm{diag}(\sigma_{T}^{2})XA  + \underbrace{ A'\underline{V}^{-1}A}_{\text{merge 1}} - A'\underline{V}^{-1}\underline{A} - \underline{A}'\underline{V}^{-1}A +\underline{A}'\underline{V}^{-1}\underline{A}' + Y'\mathrm{diag}(\sigma_{T}^{2})Y  +\underline{S} 
\\
& = A' \underbrace{[X'\mathrm{diag}(\sigma_{T}^{2}) X + \underline{V}^{-1}]}_{\overline{V}^{-1}} A - \underbrace{A'X'\mathrm{diag}(\sigma_{T}^{2})X\widehat{A} }_{A'X'\mathrm{diag}(\sigma_{T}^{2})Y}  - \underbrace{\widehat{A}'X'\mathrm{diag}(\sigma_{T}^{2})XA}_{A'X'\mathrm{diag}(\sigma_{T}^{2})Y}    - A'\underline{V}^{-1}\underline{A} - \underline{A}'\underline{V}^{-1}A +\underline{A}'\underline{V}^{-1}\underline{A}' + Y'\mathrm{diag}(\sigma_{T}^{2})Y  +\underline{S} 
\\
& = A'\overline{V}^{-1}A - 2A'  [ X'\mathrm{diag}(\sigma_{T}^{2})Y + \underline{V}^{-1}\underline{A} ] + \underline{A}'\underline{V}^{-1}\underline{A}' + Y'\mathrm{diag}(\sigma_{T}^{2})Y  +\underline{S} 
\\
& = A'\overline{V}^{-1}A - 2A' \overline{V}^{-1} \underbrace{\overline{V}[X'\mathrm{diag}(\sigma_{T}^{2})Y + \underline{V}^{-1}\underline{A} ]}_{\overline{A}}  + \underline{A}'\underline{V}^{-1}\underline{A}' + Y'\mathrm{diag}(\sigma_{T}^{2})Y  +\underline{S} 
\\
& = A'\overline{V}^{-1}A - 2A'\overline{V}^{-1}\overline{A} +  \underline{A}'\underline{V}^{-1}\underline{A}' + Y'\mathrm{diag}(\sigma_{T}^{2})Y  +\underline{S} 
\\
& = A'\overline{V}^{-1}A - 2A'\overline{V}^{-1}\overline{A} + [ \overline{A}'\overline{V}^{-1}\overline{A} -  \overline{A}'\overline{V}^{-1}\overline{A} ]+ \underline{A}'\underline{V}^{-1}\underline{A}' + Y'\mathrm{diag}(\sigma_{T}^{2})Y  +\underline{S} \\
& = \underbrace{ A'\overline{V}^{-1}A - 2A'\overline{V}^{-1}\overline{A} + \overline{A}'\overline{V}^{-1}\overline{A}}_{\text{MN part}}  \quad
\underbrace{-  \overline{A}'\overline{V}^{-1}\overline{A} + \underline{A}'\underline{V}^{-1}\underline{A}' + Y'\mathrm{diag}(\sigma_{T}^{2})Y  +\underline{S}}_{\text{IW part}\quad \overline{S}} 
\\
\\
\\
& \qquad \text{where} \ A'X'\mathrm{diag}(\sigma_{T}^{2})X\widehat{A} = A'X'\mathrm{diag}(\sigma_{T}^{2})X(X'X)^{-1}X'Y=A'X'\mathrm{diag}(\sigma_{T}^{2})Y
\end{align} 
$$

$$
\begin{align}
\overline{V} &= (X'\mathrm{diag}(\sigma_{T}^{2})^{-1}X + \underline{V}^{-1})^{-1}\\
\overline{A} &= \overline{V}[X'\mathrm{diag}(\sigma_{T}^{2})^{-1}Y+\underline{V}^{-1}\underline{A}] \\
\overline{\nu} &= T+\underline{\nu}\\
\overline{S} &= \underline{S}+Y'\mathrm{diag}(\sigma_{T}^{2})^{-1}Y+\underline{A}'\underline{V}^{-1}\underline{A}-\overline{A}'\overline{V}^{-1}\overline{A} 
\end{align}
$$

**Step 3**:

```{r heteroskedasticity model, fig.align='center',fig.pos='H'}
#| echo: true
#| message: false
#| warning: false

# Posterior sample draw function for heteroskedasticity on basic 
posterior.draws.hetero = function (S, Y, X){
  
  ## Pre-setup 
     
  N             = ncol(Y) 
  t             = N + 1
  p             = frequency(Y)                        
  A.hat         = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  T             = dim(Y)[1] # 133
  K             = dim(X)[2] # 41

  H             = diag(T)
  sdiag(H,-1)   = -1
  HH            = 2*diag(T)
  sdiag(HH,-1)  = -1
  sdiag(HH,1)   = -1
  
  # Prior distribution (with Minnesota prior)

  A.prior           = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:t,]     = diag(N)
  V.prior           = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior           = diag(diag(Sigma.hat))
  nu.prior          = N + 2
  # 
  HH                = HH
  h0.m              = 0
  h0.v              = 1
  sigmav.s          = 1
  sigmav.nu         = 1


  # Define posterior
  posteriors    = list(                        
    H           = matrix(NA,T,S),
    sigma2      = matrix(NA,T,S),
    s           = matrix(NA,T,S),
    h0          = rep(NA,S),
    sigma.v2    = rep(NA,S),
    A           = array(NA, c((1 + N * p), N, S)),
    Sigma       = array(NA, c(N, N, S))
  )
  
  # Define auxiliary
  aux        = list(
    Y        = Y,
    X        = X,
    H        = matrix(1, T, 1),
    h0       = 0,
    sigma.v2 = 1,
    s        = matrix(1, T, 1),
    A        = matrix(0, K, N),
    Sigma    = matrix(0, N, N),             
    sigma2   = matrix(1, T, 1)
  )
  
  for (s in 1:S){
      # normal-inverse Wishard posterior 
      V.bar.inv              = t(aux$X)%*%diag(1/as.vector(aux$sigma2))%*%aux$X + diag(1/ diag(V.prior))
      V.bar                  = solve(V.bar.inv)
      A.bar                  = V.bar%*%(t(aux$X)%*%diag(1/as.vector(aux$sigma2))%*%aux$Y + diag(1/diag( V.prior))%*%A.prior) 
      nu.bar                 = T + nu.prior
      S.bar                  = S.prior + t(aux$Y)%*%diag(1/as.vector(aux$sigma2))%*%aux$Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar 

      S.bar.inv              = solve(S.bar)
      S.bar.inv              = 0.5 * (t(S.bar.inv) + S.bar.inv) # positive-definite
    
      # posterior draws for A and Sigma
      Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
      Sigma.posterior.draw   = apply(Sigma.posterior.IW, 3 ,solve)
      aux$Sigma              = array(Sigma.posterior.draw,c(N,N,1))
      A.norm                 = array(rnorm(prod(c(K,N,1))),c(K,N,1))
      L                      = t(chol(V.bar))
      aux$A                  = A.bar + L%*%A.norm[,,1]%*%chol(aux$Sigma[,,1])
    

 
      # posterior draw for sigma2   
      N             = dim(aux$Y)[2] # 10
      alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)
      sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)
      pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)
      
      Lambda        = solve(chol(aux$Sigma[,,1]))
      Z             = rowSums( ( aux$Y - aux$X %*% aux$A ) %*% Lambda ) / sqrt(N)
      Y.tilde       = as.vector(log((Z + 0.0000001)^2))
      Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])
        
      # sampling initial condition
      V.h0.bar      = 1/((1 / h0.v) + (1 / aux$sigma.v2))
      m.h0.bar      = V.h0.bar*((h0.m / h0.v) + (aux$H[1] / aux$sigma.v2))
      h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))
      aux$h0        = h0.draw
      
      # sampling sigma.v2
      sigma.v2.s    = sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)
      sigma.v2.draw = sigma.v2.s / rchisq(1, sigmav.nu + T)
      aux$sigma.v2  = sigma.v2.draw
      
      # sampling auxiliary states
      Pr.tmp        = simplify2array(lapply(1:10,function(x){
        dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])
      }))
      Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))
      s.cum         = t(apply(Pr, 1, cumsum))
      r             = matrix(rep(runif(T), 10), ncol = 10)
      ss            = apply(s.cum < r, 1, sum) + 1
      aux$s         = as.matrix(ss)
      
      # sampling log-volatilities using functions for tridiagonal precision matrix
      Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s)])
      D.inv         = Sigma.s.inv + (1 / aux$sigma.v2) * HH
      b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])
      lead.diag     = diag(D.inv)
      sub.diag      = mgcv::sdiag(D.inv, -1)
      D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)
      D.L           = diag(D.chol$ld)
      mgcv::sdiag(D.L,-1) = D.chol$sd
      x             = as.matrix(rnorm(T))
      a             = forwardsolve(D.L, b)
      draw          = backsolve(t(D.L), a + x)
      aux$H         = as.matrix(draw)
      aux$sigma2    = as.matrix(exp(draw))
        
      # output list
      posteriors$H[,s]             = aux$H
      posteriors$sigma2[,s]        = aux$sigma2
      posteriors$s[,s]             = aux$s
      posteriors$h0[s]             = aux$h0
      posteriors$sigma.v2[s]       = aux$sigma.v2
      posteriors$A[,,s]            = aux$A
      posteriors$Sigma[,,s]        = aux$Sigma
      
  }
        
  return(posteriors)
}
```

```{r Heteroskedasticity Function Applying, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false

# Setup 

S                      = 5000
kappa.1                = 1               # shrinkage for A1 to Ap
kappa.2                = 100             # shrinkage for constant 

## Function Applying
posterior.hetero       = posterior.draws.hetero(S = S, Y = Y, X = X) 

A.posterior.simu       = posterior.hetero[["A"]]
Sigma.posterior.simu   = posterior.hetero[["Sigma"]]
```

```{r Basic Model with hetero Forecasting, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false

## Two-year ahead forecasting h = 8 and with p = 4

# set up
h         = 8
N         = ncol(Y) 
Y.h       = array(NA,c(h,N,S))
p         = 4

Y.h       = sampling.predictive.density.fun(A.posterior.simu = A.posterior.simu,
                                                Sigma.posterior.simu = Sigma.posterior.simu,
                                                S = S) 

```

```{r 3d forecasting graph on basic model with hetero, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 14 3D forecasting graph on basic model with heteroskedasticity"

threed.basic.hetero.graph = threed.graph(Y.h = Y.h)

```

```{r 2d forecasting graph on basic model with hetero, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 15 2D forecasting graph on basic model with heteroskedasticity"

# twod.basic.hetero.graph = twod.graph(Y.h = Y.h)
```

```{r Basic Model with hetero forecast data transform, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false

# transform cpi into inflation rate, collect with it with infexp and cash rate

key.data.h = data.trans(Y.h = Y.h)
```

```{r Basic Model with hetero Key Data Plot, fig.align='center',fig.pos='H'}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Figure 15 Basic Model with heteroskedasticity key data plot"

key.data.basic.hetero.graph = key.data.plot(key.data.h = key.data.h)

```

## References {.unnumbered}
